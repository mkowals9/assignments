# -*- coding: utf-8 -*-
"""projekcik.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gYNIRoTgHh7S5ttHmuhQd6P_ASapks19

# **PROJEKT KORONASCIENCE**

Autorka : Marcelina Kowalska

*Import bibliotek*
"""

!pip install requests
!pip install beautifulsoup4

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import requests
import bs4
import lxml.etree as xml
import requests

"""*Tytuły z "The Guardian"*"""

articles=list()
for i in range(1,13):
  URL = "https://www.theguardian.com/culture?page="+str(i)
  web_page = bs4.BeautifulSoup(requests.get(URL, {}).text, "lxml")
  grdlinks=web_page.find_all(name='a', attrs={"data-link-name":"article"})
  for link in grdlinks: 
    articles.append(link.attrs['href'].split('/')[-1].replace("-"," "))
# print(len(articles))
# articles

"""*Tytuły z "BBC Culture" - nieużyte ze względu na zbyt małą liczbę pobranych nagłówków*"""

URL2= "http://www.bbc.com/culture"
web_page2 = bs4.BeautifulSoup(requests.get(URL2, {}).text, "lxml")
articles2=list()
bbclinks=web_page2.find_all(name='a', attrs={"data-cs-element-type":"story-promo-link"})
for link in bbclinks: 
  articles2.append(link.attrs['data-cs-element-name'].lower())
#print(len(articles2))
#articles2

"""*Tytuły z "Global News"*"""

URL3 = "https://globalnews.ca/tag/culture/"
web_page3 = bs4.BeautifulSoup(requests.get(URL3, {}).text, "lxml")
articles3=list()
gnlinks=web_page3.find_all(name="a", attrs={"class":"c-posts__inner"})
for link in gnlinks: 
  articles3.append(link.attrs['title'].lower())
#print(glbn)

"""*Tytuły z "Biography"*"""

URL4 = "https://www.biography.com/news/history-and-culture"
web_page4 = bs4.BeautifulSoup(requests.get(URL4, {}).text, "lxml")
articles4=list()
biolinks=web_page4.find_all(name="a", attrs={"class":"m-card--image-link"})
for link in biolinks: articles4.append(link.attrs['title'].lower())
# print(articles4)

"""*Tytuły z "New Yorker"*"""

articles5=list()
for i in range(1,50):
  URL5 = "https://www.newyorker.com/latest/culture/page/"+str(i)
  web_page5=bs4.BeautifulSoup(requests.get(URL5, {}).text, "lxml")
  nyrlinks=((web_page5.find_all(name="h4", attrs={"class":"River__hed___re6RP"})))
  for link in nyrlinks: articles5.append(link.contents[0].lower().lstrip())
#print(len(articles5))

"""*Utworzenie DataFrame*"""

nmbr=min([len(list(dict.fromkeys(articles))),len(list(dict.fromkeys(articles5)))])
d={'The Guardian': (list(dict.fromkeys(articles)))[0:nmbr], 
   'New Yorker':(list(dict.fromkeys(articles5)))[0:nmbr]}
dfgiants=pd.DataFrame(data=d)
nmbr2=min([len(list(dict.fromkeys(articles3))),len(list(dict.fromkeys(articles4)))])
d2={'Global News': (list(dict.fromkeys(articles3)))[0:nmbr2], 
   'Biography':(list(dict.fromkeys(articles4)))[0:nmbr2]}
dfsmall=pd.DataFrame(data=d2)

"""*Utworzenie wykresu pokazującego ilość wyrazów związanych z COVID-19*"""

def piechartfordf(datafr):
  counter=0
  counter1=0
  counter2=0
  counter3=0
  counter4=0
  counter5=0
  counter6=0
  for column in datafr:
    for title in datafr[column]:
      if "coronavirus" in title: counter+=1
      if "covid-19" in title: counter1=1
      if "isolation" in title: counter2+=1
      if "quarantine" in title: counter3+=1
      if "stayathome" in title: counter4+=1
      if "lockdown" in title: counter5+=1
      if "online" in title: counter6+=1
  labels = 'coronavirus', 'covid-19', 'isolation', 'quarantine','stayathome','lockdown','online'
  sizes = [counter, counter1,counter2,counter3,counter4,counter5,counter6]
  explode = (0, 0, 0, 0, 0,0,0)
  fig1, ax1 = plt.subplots()
  ax1.pie(sizes, explode=explode,labels=labels, autopct='%0.1f%%',
        shadow=True, startangle=90)
  ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
  plt.title("Wykres przedstawiający, które z wybranych słów związanych z koronawirusem pojawia się najczęściej\n\n")
  fig = plt.gcf()
  fig.set_size_inches(9.5,9.5)
  plt.show()

piechartfordf(dfgiants)

piechartfordf(dfsmall)

"""*Liczenie ile procentowo jest słów dotyczących pandemii w danej gazecie na ilość wszystkich użytych słów w nagłówkach*"""

def percentmagazine(collumn):
  countervirus=0
  counterquar=0
  counterisolat=0
  countercovid=0
  how_many_words_in_titles=0
  for title in collumn:
    if "coronavirus" in title : countervirus+=1
    if "quarantine" in title: counterquar+=1
    if "isolation" in title: counterisolat+=1
    if "covid-19" in title: countercovid+=1
    how_many_words_in_titles+=len(title.split())
  return [round((float(countervirus)/float(how_many_words_in_titles))*100,2),round((float(counterquar)/float(how_many_words_in_titles))*100,2),round((float(counterisolat)/float(how_many_words_in_titles))*100,2),round((float(countercovid)/float(how_many_words_in_titles))*100,2)]

percentmagazine(dfgiants['The Guardian'])

labels = ['The Guardian', 'New Yorker', 'Global News', 'Biography']
coronavirus_nmbr = [percentmagazine(dfgiants['The Guardian'])[0], percentmagazine(dfgiants['New Yorker'])[0], 
                    percentmagazine(dfsmall['Global News'])[0], percentmagazine(dfsmall['Biography'])[0]]
quarantine_nmbr = [percentmagazine(dfgiants['The Guardian'])[1], percentmagazine(dfgiants['New Yorker'])[1], 
                    percentmagazine(dfsmall['Global News'])[1], percentmagazine(dfsmall['Biography'])[1]]
isolation_nmbr = [percentmagazine(dfgiants['The Guardian'])[2], percentmagazine(dfgiants['New Yorker'])[2], 
                    percentmagazine(dfsmall['Global News'])[2], percentmagazine(dfsmall['Biography'])[2]] 
covid_nmbr = [percentmagazine(dfgiants['The Guardian'])[3], percentmagazine(dfgiants['New Yorker'])[3], 
                    percentmagazine(dfsmall['Global News'])[3], percentmagazine(dfsmall['Biography'])[3]]

x = np.arange(len(labels))
width = 0.1

fig, ax = plt.subplots()
rects1 = ax.bar(x - width, coronavirus_nmbr, width, label='Coronavirus')
rects2 = ax.bar(x + width, quarantine_nmbr, width, label='Quarantine')
rects3 = ax.bar(x + 2*width, isolation_nmbr, width, label='Isolation')
rects4 = ax.bar(x - 2*width, covid_nmbr, width, label='Covid-19')


ax.set_ylabel('Procenty')
ax.set_title('Jaki procent wszystkich słów z tytułów artykułów \nstanowiły dane słowa związane z pandemią koronawirusa\n')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autoplot(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autoplot(rects1)
autoplot(rects2)
autoplot(rects3)
autoplot(rects4)

fig.tight_layout()
fig = plt.gcf()
fig.set_size_inches(9.5,9.5)
plt.show()